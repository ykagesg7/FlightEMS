#!/usr/bin/env python3
"""
æ–°è¦CPLè©¦é¨“PDFãƒ‡ãƒ¼ã‚¿ã‚’unified_cpl_questionsãƒ†ãƒ¼ãƒ–ãƒ«ç”¨SQLã«å¤‰æ›
2025å¹´6æœˆ21æ—¥ã«è¿½åŠ ã•ã‚ŒãŸ12ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
"""

import os
import sys
import json
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

def extract_questions_from_text(text: str) -> List[Dict[str, Any]]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å•é¡Œã‚’æŠ½å‡ºï¼ˆæ—¢å­˜é–¢æ•°ã®ç°¡ç•¥ç‰ˆï¼‰"""
    
    questions = []
    
    # å•é¡Œç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢
    patterns = [
        r'å•é¡Œ?\s*(\d+)',  # å•1, å•é¡Œ1
        r'(\d+)\s*[.ï¼]\s*',  # 1. 
    ]
    
    all_matches = []
    for pattern in patterns:
        matches = list(re.finditer(pattern, text))
        all_matches.extend([(m.start(), m.group(1), m.group(0)) for m in matches])
    
    # ä½ç½®ã§ã‚½ãƒ¼ãƒˆ
    all_matches.sort(key=lambda x: x[0])
    
    # é‡è¤‡é™¤å»ï¼ˆè¿‘ã„ä½ç½®ã®å•é¡Œç•ªå·ã¯åŒä¸€ã¨ã¿ãªã™ï¼‰
    unique_matches = []
    for i, (pos, num, match_text) in enumerate(all_matches):
        if i == 0 or pos - all_matches[i-1][0] > 50:  # 50æ–‡å­—ä»¥ä¸Šé›¢ã‚Œã¦ã„ã‚‹
            unique_matches.append((pos, num, match_text))
    
    print(f"ğŸ” Pattern matches found: {len(unique_matches)}")
    
    # å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
    for i, (pos, question_num, match_text) in enumerate(unique_matches):
        try:
            # æ¬¡ã®å•é¡Œã®é–‹å§‹ä½ç½®ã‚’å–å¾—
            next_pos = unique_matches[i + 1][0] if i + 1 < len(unique_matches) else len(text)
            
            # å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content_start = pos
            content_end = min(next_pos, pos + 2000)  # æœ€å¤§2000æ–‡å­—
            content = text[content_start:content_end].strip()
            
            # çŸ­ã™ãã‚‹å†…å®¹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(content) < 20:
                continue
            
            questions.append({
                'number': int(question_num),
                'content': content,
                'position': pos
            })
            
        except (ValueError, IndexError):
            continue
    
    # å•é¡Œç•ªå·ã§ã‚½ãƒ¼ãƒˆ
    questions.sort(key=lambda x: x['number'])
    
    return questions

def classify_subject(question_text: str) -> str:
    """å•é¡Œæ–‡ã‹ã‚‰ç§‘ç›®åˆ†é¡ï¼ˆunified_cpl_questionsç”¨ï¼‰"""
    
    text_lower = question_text.lower()
    
    # èˆªç©ºå·¥å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if any(keyword in text_lower for keyword in [
        'ãƒ”ãƒˆãƒ¼', 'é™åœ§', 'è¨ˆå™¨', 'é€Ÿåº¦è¨ˆ', 'cas', 'ias', 'tas',
        'ãƒ—ãƒ­ãƒšãƒ©', 'ã‚¨ãƒ³ã‚¸ãƒ³', 'å‹•åŠ›', 'ç‡ƒæ–™', 'æ²¹åœ§', 'é›»æ°—',
        'ç¿¼', 'æ©Ÿä½“', 'æ§‹é€ ', 'ææ–™', 'å¼·åº¦', 'å¿œåŠ›',
        'èˆªç©ºåŠ›å­¦', 'æšåŠ›', 'æŠ—åŠ›', 'å¤±é€Ÿ', 'ãƒãƒƒãƒ',
        'é‡é‡', 'é‡å¿ƒ', 'è·é‡', 'ãƒãƒ©ãƒ³ã‚¹'
    ]):
        return 'èˆªç©ºå·¥å­¦'
    
    # èˆªç©ºæ°—è±¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if any(keyword in text_lower for keyword in [
        'æ°—æ¸©', 'æ°—åœ§', 'æ¹¿åº¦', 'éœ²ç‚¹', 'é›²', 'éœ§', 'é›¨', 'é›ª',
        'é¢¨', 'ä¹±æ°—æµ', 'é›·', 'å°é¢¨', 'å‰ç·š', 'é«˜æ°—åœ§', 'ä½æ°—åœ§',
        'é€†è»¢', 'å¯¾æµ', 'å®‰å®š', 'ä¸å®‰å®š', 'å¤§æ°—', 'æ°—è±¡',
        'è¦–ç¨‹', 'icao', 'æ¨™æº–å¤§æ°—'
    ]):
        return 'èˆªç©ºæ°—è±¡'
    
    # ç©ºä¸­èˆªæ³•ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if any(keyword in text_lower for keyword in [
        'èˆªæ³•', 'gps', 'vor', 'dme', 'ils', 'rnav',
        'ç£æ–¹ä½', 'çœŸæ–¹ä½', 'åå·®', 'è‡ªå·®', 'ã‚³ãƒ³ãƒ‘ã‚¹',
        'åœ°å›³', 'ãƒãƒ£ãƒ¼ãƒˆ', 'åº§æ¨™', 'çµŒåº¦', 'ç·¯åº¦',
        'crm', 'äººçš„è¦å› ', 'ç–²åŠ´', 'ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¨ãƒ©ãƒ¼'
    ]):
        return 'ç©ºä¸­èˆªæ³•'
    
    # èˆªç©ºé€šä¿¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if any(keyword in text_lower for keyword in [
        'ç®¡åˆ¶', 'atc', 'äº¤ä¿¡', 'ç„¡ç·š', 'å‘¨æ³¢æ•°', 'vhf', 'hf',
        'ãƒˆãƒ©ãƒ³ã‚¹ãƒãƒ³ãƒ€', 'squawk', 'ãƒ¬ãƒ¼ãƒ€ãƒ¼',
        'é£›è¡Œè¨ˆç”»', 'fir', 'ã‚³ãƒ¼ãƒ«ã‚µã‚¤ãƒ³', 'ç®¡åˆ¶åœ',
        'é€²å…¥', 'å‡ºç™º', 'ç€é™¸', 'é›¢é™¸'
    ]):
        return 'èˆªç©ºé€šä¿¡'
    
    # èˆªç©ºæ³•è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if any(keyword in text_lower for keyword in [
        'èˆªç©ºæ³•', 'è¦å‰‡', 'æ¡ç´„', 'icao', 'å›½éš›',
        'å…è¨±', 'è³‡æ ¼', 'åŒ»å­¦é©æ€§', 'èº«ä½“æ¤œæŸ»',
        'é£›è¡Œè¦å‰‡', 'vfr', 'ifr', 'æœ€ä½æ°—è±¡æ¡ä»¶',
        'ç¦æ­¢åŒºåŸŸ', 'åˆ¶é™åŒºåŸŸ', 'å±é™ºåŒºåŸŸ',
        'èˆªç©ºæ©Ÿç™»éŒ²', 'è€ç©ºè¨¼æ˜'
    ]):
        return 'èˆªç©ºæ³•è¦'
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ãªç§‘ç›®ï¼‰
    return 'èˆªç©ºå·¥å­¦'

def classify_sub_category(question_text: str, main_subject: str) -> str:
    """ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
    
    text_lower = question_text.lower()
    
    if main_subject == 'èˆªç©ºå·¥å­¦':
        if any(keyword in text_lower for keyword in ['ãƒ”ãƒˆãƒ¼', 'è¨ˆå™¨', 'é€Ÿåº¦è¨ˆ', 'é«˜åº¦è¨ˆ']):
            return 'èˆªç©ºè¨ˆå™¨'
        elif any(keyword in text_lower for keyword in ['ãƒ—ãƒ­ãƒšãƒ©', 'ã‚¨ãƒ³ã‚¸ãƒ³', 'å‹•åŠ›']):
            return 'å‹•åŠ›è£…ç½®'
        elif any(keyword in text_lower for keyword in ['ç¿¼', 'æ©Ÿä½“', 'æ§‹é€ ']):
            return 'èˆªç©ºæ©Ÿæ§‹é€ '
        elif any(keyword in text_lower for keyword in ['æšåŠ›', 'æŠ—åŠ›', 'èˆªç©ºåŠ›å­¦']):
            return 'èˆªç©ºåŠ›å­¦'
        else:
            return 'èˆªç©ºæ©Ÿè£…å‚™'
    
    elif main_subject == 'èˆªç©ºæ°—è±¡':
        if any(keyword in text_lower for keyword in ['å¤§æ°—', 'æ°—æ¸©', 'æ°—åœ§']):
            return 'å¤§æ°—ã®ç‰©ç†'
        elif any(keyword in text_lower for keyword in ['é¢¨', 'å‰ç·š', 'é«˜æ°—åœ§']):
            return 'å¤§æ°—ã®é‹å‹•'
        elif any(keyword in text_lower for keyword in ['é›²', 'ä¹±æ°—æµ', 'é›·']):
            return 'é«˜å±¤æ°—è±¡ã¨æ°—è±¡éšœå®³'
        else:
            return 'æ°—è±¡æƒ…å ±'
    
    elif main_subject == 'ç©ºä¸­èˆªæ³•':
        if any(keyword in text_lower for keyword in ['vor', 'dme', 'gps', 'èˆªæ³•']):
            return 'èˆªæ³•'
        elif any(keyword in text_lower for keyword in ['crm', 'äººçš„è¦å› ']):
            return 'äººé–“ã®èƒ½åŠ›åŠã³é™ç•Œã«é–¢ã™ã‚‹ä¸€èˆ¬çŸ¥è­˜'
        else:
            return 'é‹èˆªæ–¹å¼ã«é–¢ã™ã‚‹ä¸€èˆ¬çŸ¥è­˜'
    
    elif main_subject == 'èˆªç©ºé€šä¿¡':
        if any(keyword in text_lower for keyword in ['ç®¡åˆ¶', 'atc']):
            return 'ç®¡åˆ¶æ¥­å‹™'
        else:
            return 'èˆªç©ºäº¤é€šæ¥­å‹™'
    
    elif main_subject == 'èˆªç©ºæ³•è¦':
        if any(keyword in text_lower for keyword in ['æ¡ç´„', 'icao', 'å›½éš›']):
            return 'å›½éš›æ¡ç´„'
        else:
            return 'èˆªç©ºæ³•åŠã³èˆªç©ºæ³•æ–½è¡Œè¦å‰‡'
    
    return main_subject

def estimate_difficulty(question_text: str) -> int:
    """é›£æ˜“åº¦æ¨å®šï¼ˆ1-5ï¼‰"""
    
    text_length = len(question_text)
    text_lower = question_text.lower()
    
    # åŸºæœ¬é›£æ˜“åº¦
    difficulty = 3
    
    # é•·ã„å•é¡Œæ–‡ã¯é›£æ˜“åº¦ãŒé«˜ã„
    if text_length > 400:
        difficulty += 1
    elif text_length < 150:
        difficulty -= 1
    
    # è¤‡é›‘ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆ
    complex_keywords = ['è¨ˆç®—', 'å›³è¡¨', 'è¤‡æ•°', 'çµ„ã¿åˆã‚ã›', 'ï½ï¼ˆï½„ï¼‰', 'ã„ãã¤']
    if any(keyword in text_lower for keyword in complex_keywords):
        difficulty += 1
    
    # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆ
    basic_keywords = ['æ­£ã—ã„', 'èª¤ã‚Š', 'ã©ã‚Œã‹', 'èª¬æ˜']
    if all(keyword in text_lower for keyword in basic_keywords):
        difficulty -= 1
    
    return max(1, min(5, difficulty))

def calculate_importance_score(subject: str, difficulty: int, text_length: int) -> float:
    """é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ1.0-10.0ï¼‰"""
    
    # åŸºæœ¬ã‚¹ã‚³ã‚¢
    base_score = 5.0
    
    # ç§‘ç›®ã«ã‚ˆã‚‹é‡ã¿
    subject_weights = {
        'èˆªç©ºå·¥å­¦': 1.2,
        'èˆªç©ºæ°—è±¡': 1.1,
        'ç©ºä¸­èˆªæ³•': 1.0,
        'èˆªç©ºé€šä¿¡': 0.9,
        'èˆªç©ºæ³•è¦': 0.8
    }
    
    base_score *= subject_weights.get(subject, 1.0)
    
    # é›£æ˜“åº¦ã«ã‚ˆã‚‹èª¿æ•´
    base_score += (difficulty - 3) * 0.5
    
    # æ–‡ç« é•·ã«ã‚ˆã‚‹èª¿æ•´
    if text_length > 300:
        base_score += 0.3
    elif text_length < 100:
        base_score -= 0.3
    
    return round(max(1.0, min(10.0, base_score)), 1)

def generate_tags(question_text: str, subject: str) -> List[str]:
    """ã‚¿ã‚°ç”Ÿæˆ"""
    
    tags = ['CPL', '2024å¹´', subject]
    
    text_lower = question_text.lower()
    
    # ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ãã‚¿ã‚°
    keyword_tags = {
        'è¨ˆå™¨': ['è¨ˆå™¨é£›è¡Œ'],
        'æ°—è±¡': ['å¤©å€™', 'æ°—è±¡ç¾è±¡'],
        'ç®¡åˆ¶': ['ATC', 'äº¤ä¿¡'],
        'èˆªæ³•': ['ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³'],
        'æ³•è¦': ['è¦å‰‡', 'æ³•ä»¤']
    }
    
    for keyword, tag_list in keyword_tags.items():
        if keyword in text_lower:
            tags.extend(tag_list)
    
    return list(set(tags))  # é‡è¤‡é™¤å»

def create_unified_insert_sql(questions: List[Dict[str, Any]], source_file: str, year: int, month: int) -> str:
    """unified_cpl_questionsç”¨ã®INSERT SQLã‚’ç”Ÿæˆ"""
    
    if not questions:
        return ""
    
    sql_lines = []
    sql_lines.append(f"-- CPLè©¦é¨“ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ ({source_file})")
    sql_lines.append(f"-- æŠ•å…¥ä»¶æ•°: {len(questions)} å•")
    sql_lines.append(f"-- ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    sql_lines.append("")
    
    sql_lines.append("INSERT INTO unified_cpl_questions (")
    sql_lines.append("    main_subject, sub_subject, detailed_topic,")
    sql_lines.append("    question_text, options, correct_answer, explanation,")
    sql_lines.append("    source_documents, difficulty_level, importance_score,")
    sql_lines.append("    appearance_frequency, verification_status, quality_score,")
    sql_lines.append("    tags, exam_type, created_at, updated_at")
    sql_lines.append(") VALUES")
    
    value_lines = []
    
    for i, question in enumerate(questions):
        question_text = question.get('content', '')
        
        # ç§‘ç›®åˆ†é¡
        main_subject = classify_subject(question_text)
        sub_subject = classify_sub_category(question_text, main_subject)
        
        # é›£æ˜“åº¦ãƒ»é‡è¦åº¦
        difficulty = estimate_difficulty(question_text)
        importance = calculate_importance_score(main_subject, difficulty, len(question_text))
        
        # ã‚¿ã‚°ç”Ÿæˆ
        tags = generate_tags(question_text, main_subject)
        
        # ã‚½ãƒ¼ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        source_metadata = {
            "sources": [
                {
                    "type": "official_exam",
                    "year": year,
                    "month": month,
                    "question_num": question.get('number', i + 1),
                    "file": source_file,
                    "extraction_confidence": 0.9
                }
            ],
            "weight": 3.0,
            "originality": "official"
        }
        
        # SQLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
        def escape_sql(text):
            if text is None:
                return 'NULL'
            return "'" + str(text).replace("'", "''") + "'"
        
        value_line = f"""    (
        {escape_sql(main_subject)},
        {escape_sql(sub_subject)},
        NULL,
        {escape_sql(question_text[:1000])},  -- æœ€å¤§1000æ–‡å­—ã«åˆ¶é™
        '[]'::jsonb,  -- é¸æŠè‚¢æƒ…å ±ãªã—
        NULL,  -- æ­£è§£ãªã—
        NULL,  -- è§£èª¬ãªã—
        '{json.dumps(source_metadata, ensure_ascii=False)}'::jsonb,
        {difficulty},
        {importance},
        1,
        'pending',
        0.90,
        ARRAY{tags},
        'CPL',
        now(),
        now()
    )"""
        
        value_lines.append(value_line)
    
    sql_lines.append(',\n'.join(value_lines))
    sql_lines.append(";")
    
    return '\n'.join(sql_lines)

def process_new_pdfs():
    """æ–°è¦PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
    
    print("ğŸš€ æ–°è¦CPLè©¦é¨“PDFãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # æ–°è¦å¤‰æ›ã•ã‚ŒãŸMarkdownãƒ•ã‚¡ã‚¤ãƒ«
    new_files = [
        'real_pdf_202311_CPLTest.md',
        'real_pdf_202401_CPLTest.md',
        'real_pdf_202403_CPLTest.md',
        'real_pdf_202405_CPLTest.md',
        'real_pdf_202407_CPLTest.md',
        'real_pdf_202409_CPLTest.md',
        'real_pdf_202411_CPLTest.md',
        'real_pdf_202501_CPLTest.md',
        'real_pdf_202503_CPLTest.md',
        'real_pdf_202505_CPLTest.md',
        'real_pdf_202507_CPLTest.md',
        'real_pdf_202509_CPLTest.md'
    ]
    
    converted_md_dir = Path("./cpl_exam_data/converted_md")
    scripts_dir = Path("./scripts")
    
    total_questions = 0
    processed_files = 0
    
    for md_file in new_files:
        md_path = converted_md_dir / md_file
        
        if not md_path.exists():
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {md_file}")
            continue
        
        print(f"\nğŸ“– å‡¦ç†ä¸­: {md_file}")
        
        # å¹´æœˆã‚’æŠ½å‡º
        import re
        match = re.search(r'(\d{4})(\d{2})_CPLTest', md_file)
        if match:
            year = int(match.group(1))
            month = int(match.group(2))
        else:
            year, month = 2024, 1
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # å•é¡Œã‚’æŠ½å‡º
        questions = extract_questions_from_text(content)
        
        if not questions:
            print(f"   âŒ å•é¡ŒãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            continue
        
        print(f"   âœ… {len(questions)}å•ã‚’æŠ½å‡º")
        
        # SQLã‚’ç”Ÿæˆ
        source_file = md_file.replace('real_pdf_', '').replace('.md', '.pdf')
        sql_content = create_unified_insert_sql(questions, source_file, year, month)
        
        # SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        sql_file = scripts_dir / f"unified_insert_{source_file.replace('.pdf', '')}.sql"
        
        with open(sql_file, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        print(f"   ğŸ’¾ SQLä¿å­˜: {sql_file.name}")
        
        total_questions += len(questions)
        processed_files += 1
    
    print(f"\nğŸ‰ å‡¦ç†å®Œäº†!")
    print(f"   å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {processed_files}")
    print(f"   ç·å•é¡Œæ•°: {total_questions}")
    print(f"   å¹³å‡å•é¡Œæ•°: {total_questions/max(processed_files, 1):.1f}å•/ãƒ•ã‚¡ã‚¤ãƒ«")

if __name__ == "__main__":
    process_new_pdfs() 